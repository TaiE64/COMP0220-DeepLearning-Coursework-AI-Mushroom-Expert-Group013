#!/usr/bin/env python3
"""
ğŸŒ çˆ¬å–è˜‘è‡ç›¸å…³å»¶å±•æ€§è¯é¢˜çš„ç»´åŸºç™¾ç§‘å†…å®¹
æ‰©å±•çŸ¥è¯†åº“ï¼šèŒç±»å­¦ã€çƒ¹é¥ªã€åŒ»å­¦ã€ç”Ÿæ€å­¦ç­‰
"""

import wikipediaapi
import json
import time
import os

# ================= é…ç½® =================
OUTPUT_FILE = "data/raw_data_source/related_topics_wiki_data.json"
SLEEP_BETWEEN_REQUESTS = 1  # é¿å…è¢«å°IP

# ç›¸å…³ä¸»é¢˜åˆ—è¡¨ï¼ˆç»´åŸºç™¾ç§‘é¡µé¢åç§°ï¼‰
RELATED_TOPICS = [
    # === åŸºç¡€ç”Ÿç‰©å­¦ ===
    "Fungus",                      # çœŸèŒ
    "Mycelium",                    # èŒä¸
    "Spore",                       # å­¢å­
    "Basidiomycota",               # æ‹…å­èŒé—¨
    "Ascomycota",                  # å­å›ŠèŒé—¨
    "Fungal_life_cycle",           # çœŸèŒç”Ÿå‘½å‘¨æœŸ

    # === è˜‘è‡ç›¸å…³ ===
    "Mushroom",                    # è˜‘è‡ï¼ˆæ€»è¿°ï¼‰
    "Edible_mushroom",             # é£Ÿç”¨èŒ
    "Mushroom_poisoning",          # è˜‘è‡ä¸­æ¯’
    "Mushroom_hunting",            # é‡‡è˜‘è‡/è§…è‡
    "Medicinal_mushrooms",         # è¯ç”¨è˜‘è‡
    "Psychedelic_mushroom",        # è‡´å¹»è˜‘è‡

    # === æ ½åŸ¹ä¸äº§ä¸š ===
    "Mushroom_cultivation",        # è˜‘è‡æ ½åŸ¹
    "Fungiculture",                # çœŸèŒåŸ¹å…»
    "Mushroom_spawn",              # èŒç§

    # === çƒ¹é¥ªä¸åº”ç”¨ ===
    "Mushroom_soup",               # è˜‘è‡æ±¤
    "Shiitake",                    # é¦™è‡ï¼ˆå¸¸è§é£Ÿç”¨èŒï¼‰
    "Button_mushroom",             # åŒå­¢è˜‘è‡
    "Oyster_mushroom",             # å¹³è‡
    "Enoki_mushroom",              # é‡‘é’ˆè‡

    # === ç”Ÿæ€ä¸ç§‘å­¦ ===
    "Mycology",                    # çœŸèŒå­¦
    "Mycorrhiza",                  # èŒæ ¹
    "Decomposer",                  # åˆ†è§£è€…
    "Fungal_ecology",              # çœŸèŒç”Ÿæ€å­¦

    # === æ¯’ç†å­¦ ===
    "Amatoxin",                    # é¹…è†æ¯’ç´ 
    "Muscimol",                    # è‡è•ˆé†‡
    "Psilocybin",                  # è£¸ç›–è‡ç´ 
    "Mushroom_toxin",              # è˜‘è‡æ¯’ç´ 

    # === è¯†åˆ«ä¸å®‰å…¨ ===
    "Mushroom_identification",     # è˜‘è‡è¯†åˆ«
    "Lookalike_mushroom",          # ç›¸ä¼¼è˜‘è‡
    "Foraging",                    # è§…é£Ÿ
]

# ========================================

def fetch_wikipedia_content(wiki_wiki, topic):
    """
    çˆ¬å–ç»´åŸºç™¾ç§‘é¡µé¢å†…å®¹ï¼ˆä½¿ç”¨ wikipedia-apiï¼‰
    """
    try:
        # è·å–é¡µé¢
        page = wiki_wiki.page(topic)

        if not page.exists():
            return None

        # æå–æ ‡é¢˜
        title = page.title

        # æå–æ‘˜è¦ï¼ˆç¬¬ä¸€æ®µï¼‰
        summary = page.summary[:1200] if page.summary else ""

        # æå–ç« èŠ‚å†…å®¹
        sections = {}

        def extract_sections(section, depth=0, max_depth=2):
            """é€’å½’æå–ç« èŠ‚å†…å®¹ï¼ˆåªå–å‰2å±‚ï¼‰"""
            if depth >= max_depth:
                return

            for s in section.sections:
                # è·³è¿‡å¸¸è§çš„æ— ç”¨ç« èŠ‚
                if s.title.lower() in ['references', 'external links', 'see also', 'notes', 'bibliography']:
                    continue

                # ä¿å­˜ç« èŠ‚å†…å®¹
                if s.text and len(s.text) > 100:
                    sections[s.title] = s.text[:1200]  # é™åˆ¶é•¿åº¦

                    # åªä¿å­˜å‰5ä¸ªç« èŠ‚
                    if len(sections) >= 6:
                        return

                # é€’å½’æå–å­ç« èŠ‚
                extract_sections(s, depth + 1, max_depth)

                if len(sections) >= 5:
                    return

        # ä»æ ¹ç« èŠ‚å¼€å§‹æå–
        extract_sections(page, depth=0)

        return {
            "topic": title,
            "wiki_url": page.fullurl,
            "summary": summary,
            "sections": sections
        }

    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")
        return None

def main():
    print("=" * 80)
    print("ğŸŒ çˆ¬å–è˜‘è‡ç›¸å…³å»¶å±•æ€§è¯é¢˜")
    print("=" * 80)
    print(f"\nğŸ“‹ è®¡åˆ’çˆ¬å– {len(RELATED_TOPICS)} ä¸ªä¸»é¢˜\n")

    # åˆå§‹åŒ– Wikipedia API
    wiki_wiki = wikipediaapi.Wikipedia(
        language='en',
        user_agent='MushroomKnowledgeBot/1.0 (educational purposes)'
    )

    all_data = []
    success_count = 0

    for idx, topic in enumerate(RELATED_TOPICS, 1):
        print(f"[{idx}/{len(RELATED_TOPICS)}] çˆ¬å–: {topic.replace('_', ' ')}", end=" ", flush=True)

        data = fetch_wikipedia_content(wiki_wiki, topic)

        if data:
            all_data.append(data)
            success_count += 1
            print(f"âœ… ({len(data['summary'])} å­—ç¬¦)")
        else:
            print("âš ï¸  è·³è¿‡")

        time.sleep(SLEEP_BETWEEN_REQUESTS)

    # ä¿å­˜æ•°æ®
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print("\n" + "=" * 80)
    print(f"âœ… å®Œæˆï¼æˆåŠŸçˆ¬å– {success_count}/{len(RELATED_TOPICS)} ä¸ªä¸»é¢˜")
    print(f"ğŸ“ ä¿å­˜åˆ°: {OUTPUT_FILE}")
    print("=" * 80)

    # é¢„è§ˆ
    print("\nğŸ“ æ•°æ®é¢„è§ˆ:")
    for i, item in enumerate(all_data[:3], 1):
        print(f"\nã€{i}ã€‘{item['topic']}")
        print(f"   æ‘˜è¦: {item['summary'][:150]}...")
        print(f"   ç« èŠ‚æ•°: {len(item['sections'])}")

if __name__ == "__main__":
    main()
