#!/usr/bin/env python3
"""
ğŸ” RAG æŸ¥è¯¢ç³»ç»Ÿ
ä½¿ç”¨å‘é‡æ•°æ®åº“æ£€ç´¢ + LLM ç”Ÿæˆç­”æ¡ˆ
"""

import sys
import chromadb
from chromadb.utils import embedding_functions
import subprocess

# ================= é…ç½® =================
CHROMA_DB_PATH = "data/RAG_dataset/chroma_db"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
OLLAMA_MODEL = "qwen2.5vl:32b"  # ä½ çš„æœ¬åœ°æ¨¡å‹
TOP_K = 5  # æ£€ç´¢å‰5ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
# ========================================

def retrieve_relevant_docs(query: str, top_k: int = TOP_K):
    """ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    print(f"ğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£...")

    try:
        client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=EMBEDDING_MODEL
        )

        collection = client.get_collection(
            name="mushroom_knowledge",
            embedding_function=embedding_function
        )

        results = collection.query(
            query_texts=[query],
            n_results=top_k
        )

        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        distances = results['distances'][0]

        print(f"âœ… æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£\n")

        # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
        retrieved_docs = []
        for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances), 1):
            topic = metadata.get('topic', 'Unknown')
            section = metadata.get('section', 'Unknown')
            similarity = 1 - distance  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦

            print(f"ğŸ“„ [{i}] {topic} - {section} (ç›¸ä¼¼åº¦: {similarity:.2%})")
            print(f"    {doc[:150]}...\n")

            retrieved_docs.append({
                "text": doc,
                "metadata": metadata,
                "similarity": similarity
            })

        return retrieved_docs

    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python data/RAG_dataset/build_rag_database.py")
        return []

def generate_answer_with_llm(query: str, context_docs: list):
    """ä½¿ç”¨ LLM åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ"""
    print("ğŸ¤– ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ...\n")

    # æ„å»ºä¸Šä¸‹æ–‡
    context = "\n\n---\n\n".join([
        f"[Document {i+1}]\n{doc['text']}"
        for i, doc in enumerate(context_docs)
    ])

    # æ„å»º prompt
    prompt = f"""You are a knowledgeable mushroom expert assistant. Answer the user's question based ONLY on the provided context documents.

Context Documents:
{context}

User Question: {query}

Instructions:
1. Answer based ONLY on the information in the context documents
2. If the context doesn't contain enough information, say "I don't have enough information in my knowledge base to answer that."
3. Be concise but informative (3-5 sentences)
4. Cite which document(s) you used if relevant
5. Use a friendly, conversational tone

Answer:"""

    # è°ƒç”¨ Ollama
    try:
        result = subprocess.run(
            ["ollama", "run", OLLAMA_MODEL, prompt],
            capture_output=True,
            text=True,
            timeout=60
        )

        answer = result.stdout.strip()
        return answer

    except Exception as e:
        return f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}"

def main():
    print("=" * 80)
    print("ğŸ” è˜‘è‡çŸ¥è¯† RAG æŸ¥è¯¢ç³»ç»Ÿ")
    print("=" * 80)

    # è·å–æŸ¥è¯¢
    if len(sys.argv) < 2:
        print("\nç”¨æ³•: python data/RAG_dataset/rag_query.py \"ä½ çš„é—®é¢˜\"")
        print("\nç¤ºä¾‹æŸ¥è¯¢:")
        print('  python data/RAG_dataset/rag_query.py "What is Amanita muscaria?"')
        print('  python data/RAG_dataset/rag_query.py "How to identify poisonous mushrooms?"')
        print('  python data/RAG_dataset/rag_query.py "What is mycelium and how does it grow?"')
        return

    query = " ".join(sys.argv[1:])
    print(f"\nâ“ é—®é¢˜: {query}\n")

    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    docs = retrieve_relevant_docs(query)

    if not docs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return

    # 2. ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
    answer = generate_answer_with_llm(query, docs)

    # 3. æ˜¾ç¤ºç­”æ¡ˆ
    print("=" * 80)
    print("ğŸ’¬ ç­”æ¡ˆ:")
    print("=" * 80)
    print(answer)
    print("\n" + "=" * 80)

    # 4. æ˜¾ç¤ºæ¥æº
    print("\nğŸ“š ä¿¡æ¯æ¥æº:")
    for i, doc in enumerate(docs, 1):
        metadata = doc['metadata']
        topic = metadata.get('topic', 'Unknown')
        url = metadata.get('url', '')
        print(f"  [{i}] {topic}")
        if url:
            print(f"      {url}")

if __name__ == "__main__":
    main()
