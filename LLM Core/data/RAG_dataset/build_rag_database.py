#!/usr/bin/env python3
"""
ğŸ” æ„å»º RAG å‘é‡æ•°æ®åº“
å°†è˜‘è‡çŸ¥è¯†åº“è½¬æ¢ä¸ºå¯æ£€ç´¢çš„å‘é‡æ•°æ®åº“
"""

import json
import os
from typing import List, Dict
import chromadb
from chromadb.utils import embedding_functions
import hashlib

# ================= é…ç½® =================
# åŸå§‹æ•°æ®æº
MUSHROOM_DATA = "data/raw_data_source/raw_mushroom_wiki_data.json"
RELATED_TOPICS_DATA = "data/raw_data_source/related_topics_wiki_data.json"

# å‘é‡æ•°æ®åº“å­˜å‚¨ä½ç½®
CHROMA_DB_PATH = "data/RAG_dataset/chroma_db"

# åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨å…è´¹çš„ sentence-transformersï¼‰
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # è½»é‡çº§ï¼Œå¿«é€Ÿ
# å…¶ä»–é€‰æ‹©:
# - "all-mpnet-base-v2"  # æ›´å‡†ç¡®ï¼Œä½†ç¨æ…¢
# - "paraphrase-multilingual-MiniLM-L12-v2"  # æ”¯æŒä¸­æ–‡
# ========================================

def generate_unique_id(prefix: str, text: str) -> str:
    """ç”Ÿæˆå”¯ä¸€ IDï¼ˆä½¿ç”¨å“ˆå¸Œé¿å…é‡å¤ï¼‰"""
    hash_val = hashlib.md5(text.encode()).hexdigest()[:8]
    return f"{prefix}_{hash_val}"

def load_mushroom_data() -> List[Dict]:
    """åŠ è½½è˜‘è‡æ•°æ®"""
    print("ğŸ“š åŠ è½½è˜‘è‡æ•°æ®...")

    documents = []

    # 1. åŠ è½½å…·ä½“è˜‘è‡å“ç§
    try:
        with open(MUSHROOM_DATA, 'r', encoding='utf-8') as f:
            mushrooms = json.load(f)

        # å»é‡è˜‘è‡ï¼ˆåŸºäºè¯é¢˜åç§°ï¼‰
        seen_mushrooms = set()
        unique_mushrooms = []
        for mushroom in mushrooms:
            mushroom_name = mushroom.get('topic', 'Unknown')
            if mushroom_name not in seen_mushrooms:
                seen_mushrooms.add(mushroom_name)
                unique_mushrooms.append(mushroom)
        
        if len(mushrooms) != len(unique_mushrooms):
            print(f"  âš ï¸  å‘ç° {len(mushrooms) - len(unique_mushrooms)} ä¸ªé‡å¤è˜‘è‡ï¼Œå·²å»é‡")
        
        mushrooms = unique_mushrooms

        for mushroom in mushrooms:
            topic = mushroom.get('topic', 'Unknown')
            summary = mushroom.get('summary', '')
            sections = mushroom.get('sections', {})

            # æ·»åŠ æ‘˜è¦ä½œä¸ºä¸€ä¸ªæ–‡æ¡£
            if summary:
                doc_text = f"{topic}\n\n{summary}"
                documents.append({
                    "id": generate_unique_id("mushroom", doc_text),
                    "text": f"{topic}\n\n{summary}",
                    "metadata": {
                        "type": "mushroom",
                        "topic": topic,
                        "section": "summary",
                        "url": mushroom.get('wiki_url', '')
                    }
                })

            # æ¯ä¸ªç« èŠ‚ä½œä¸ºç‹¬ç«‹æ–‡æ¡£
            for section_name, section_content in sections.items():
                if len(section_content) > 100:  # è·³è¿‡å¤ªçŸ­çš„
                    doc_text = f"{topic} - {section_name}\n\n{section_content}"
                    documents.append({
                        "id": generate_unique_id("mushroom", doc_text),
                        "text": doc_text,
                        "metadata": {
                            "type": "mushroom",
                            "topic": topic,
                            "section": section_name,
                            "url": mushroom.get('wiki_url', '')
                        }
                    })

        print(f"  âœ… åŠ è½½äº† {len(mushrooms)} ç§è˜‘è‡")

    except FileNotFoundError:
        print(f"  âš ï¸  æ‰¾ä¸åˆ° {MUSHROOM_DATA}")

    # 2. åŠ è½½å»¶å±•è¯é¢˜
    try:
        with open(RELATED_TOPICS_DATA, 'r', encoding='utf-8') as f:
            topics = json.load(f)

        # å»é‡è¯é¢˜ï¼ˆåŸºäºè¯é¢˜åç§°ï¼‰
        seen_topics = set()
        unique_topics = []
        for topic_data in topics:
            topic_name = topic_data.get('topic', 'Unknown')
            if topic_name not in seen_topics:
                seen_topics.add(topic_name)
                unique_topics.append(topic_data)
        
        if len(topics) != len(unique_topics):
            print(f"  âš ï¸  å‘ç° {len(topics) - len(unique_topics)} ä¸ªé‡å¤è¯é¢˜ï¼Œå·²å»é‡")
        
        topics = unique_topics

        for topic_data in topics:
            topic = topic_data.get('topic', 'Unknown')
            summary = topic_data.get('summary', '')
            sections = topic_data.get('sections', {})

            # æ·»åŠ æ‘˜è¦
            if summary:
                doc_text = f"{topic}\n\n{summary}"
                documents.append({
                    "id": generate_unique_id("topic", doc_text),
                    "text": f"{topic}\n\n{summary}",
                    "metadata": {
                        "type": "general_topic",
                        "topic": topic,
                        "section": "summary",
                        "url": topic_data.get('wiki_url', '')
                    }
                })

            # ç« èŠ‚
            for section_name, section_content in sections.items():
                if len(section_content) > 100:
                    doc_text = f"{topic} - {section_name}\n\n{section_content}"
                    documents.append({
                        "id": generate_unique_id("topic", doc_text),
                        "text": doc_text,
                        "metadata": {
                            "type": "general_topic",
                            "topic": topic,
                            "section": section_name,
                            "url": topic_data.get('wiki_url', '')
                        }
                    })

        print(f"  âœ… åŠ è½½äº† {len(topics)} ä¸ªé€šç”¨è¯é¢˜")

    except FileNotFoundError:
        print(f"  âš ï¸  æ‰¾ä¸åˆ° {RELATED_TOPICS_DATA}")

    print(f"\nğŸ“Š åŸå§‹æ–‡æ¡£: {len(documents)} ä¸ª")

    # å»é‡ï¼ˆåŸºäº IDï¼‰
    seen_ids = set()
    unique_documents = []
    duplicates = 0

    for doc in documents:
        if doc['id'] not in seen_ids:
            seen_ids.add(doc['id'])
            unique_documents.append(doc)
        else:
            duplicates += 1

    if duplicates > 0:
        print(f"âš ï¸  å‘ç° {duplicates} ä¸ªé‡å¤æ–‡æ¡£ï¼Œå·²å»é‡")

    print(f"ğŸ“Š å»é‡å: {len(unique_documents)} ä¸ªæ–‡æ¡£å—")
    return unique_documents

def build_vector_database(documents: List[Dict]):
    """æ„å»ºå‘é‡æ•°æ®åº“"""
    print("\nğŸ”§ æ„å»ºå‘é‡æ•°æ®åº“...")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(CHROMA_DB_PATH, exist_ok=True)

    # åˆå§‹åŒ– Chroma å®¢æˆ·ç«¯
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

    # ä½¿ç”¨ sentence-transformers åµŒå…¥æ¨¡å‹
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )

    # åˆ›å»ºæˆ–è·å–é›†åˆ
    try:
        # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒ
        client.delete_collection(name="mushroom_knowledge")
        print("  ğŸ—‘ï¸  åˆ é™¤æ—§é›†åˆ")
    except:
        pass

    collection = client.create_collection(
        name="mushroom_knowledge",
        embedding_function=embedding_function,
        metadata={"description": "Mushroom and mycology knowledge base"}
    )

    # æ‰¹é‡æ·»åŠ æ–‡æ¡£ï¼ˆChroma æ¨èæ‰¹é‡æ’å…¥ä»¥æé«˜æ•ˆç‡ï¼‰
    batch_size = 100
    total_batches = (len(documents) + batch_size - 1) // batch_size

    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]

        ids = [doc['id'] for doc in batch]
        texts = [doc['text'] for doc in batch]
        metadatas = [doc['metadata'] for doc in batch]

        collection.add(
            ids=ids,
            documents=texts,
            metadatas=metadatas
        )

        batch_num = i // batch_size + 1
        print(f"  âœ… å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ–‡æ¡£)")

    print(f"\nğŸ‰ å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    print(f"ğŸ“ ä½ç½®: {CHROMA_DB_PATH}")
    print(f"ğŸ“Š æ–‡æ¡£æ€»æ•°: {collection.count()}")

def test_retrieval():
    """æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")

    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )

    collection = client.get_collection(
        name="mushroom_knowledge",
        embedding_function=embedding_function
    )

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "What is Amanita muscaria?",
        "How to identify poisonous mushrooms?",
        "What is mycelium?",
        "Can I eat Death Cap mushrooms?"
    ]

    for query in test_queries:
        print(f"\nâ“ æŸ¥è¯¢: {query}")

        results = collection.query(
            query_texts=[query],
            n_results=3  # è¿”å›å‰3ä¸ªæœ€ç›¸å…³çš„ç»“æœ
        )

        print("ğŸ“„ æ£€ç´¢ç»“æœ:")
        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):
            topic = metadata.get('topic', 'Unknown')
            section = metadata.get('section', 'Unknown')
            print(f"  [{i}] {topic} ({section})")
            print(f"      {doc[:100]}...")

# ==================== ä¸»é€»è¾‘ ====================
def main():
    print("=" * 80)
    print("ğŸ” æ„å»ºè˜‘è‡çŸ¥è¯† RAG å‘é‡æ•°æ®åº“")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    documents = load_mushroom_data()

    if not documents:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä»¥å¤„ç†ï¼")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ:")
        print("   - python data/scripts/scrape_mushroom_wiki.py")
        print("   - python data/scripts/scrape_related_topics.py")
        return

    # 2. æ„å»ºå‘é‡æ•°æ®åº“
    build_vector_database(documents)

    # 3. æµ‹è¯•æ£€ç´¢
    test_retrieval()

    print("\n" + "=" * 80)
    print("âœ… å®Œæˆï¼ç°åœ¨å¯ä»¥ä½¿ç”¨ RAG ç³»ç»Ÿäº†")
    print("=" * 80)
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   python data/RAG_dataset/rag_query.py \"What is Amanita muscaria?\"")

if __name__ == "__main__":
    main()
